{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Etapa 2 ‚Äî Limpieza, Enriquecimiento y EDA  \n**Proyecto Integrado V ‚Äî Ingenier√≠a de Software y Datos (IU Digital de Antioquia)**\n\n**Dataset:** `datos/estilo de vida_salud_kaggle.csv`  \n**Variables clave seleccionadas (Etapa 1):**  \n- `age`  \n- `bmi`  \n- `exercise_days_per_week`  \n- `sleep_hours`  \n- `sugar_intake_g`\n\n---\n\n## Objetivo de la etapa  \n1. Ejecutar limpieza, normalizaci√≥n y enriquecimiento del dataset con Python.  \n2. Agregar columnas de fecha, a√±o, mes y d√≠a.  \n3. Realizar an√°lisis descriptivo (EDA) de las 5 variables clave mediante estad√≠sticas y visualizaciones.  \n4. Guardar dataset enriquecido y evidencias gr√°ficas en el repositorio.  \n\n> **Nota metodol√≥gica:** si el dataset no contiene fecha, se genera una columna de fecha aleatoria (2022‚Äì2024) para an√°lisis temporal exploratorio.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Configuraci√≥n del entorno  \nEn esta secci√≥n se importan librer√≠as, se definen rutas del repositorio y variables clave.  \nCada paso queda registrado en el notebook con comentarios y salidas de evidencia (prints)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nimport re\nimport unicodedata\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# =========================\n# Configuraci√≥n base (AJUSTADA A TU REPO)\n# =========================\n# Entrada:\nRUTA_DATASET_ORIGINAL = Path(\"datos/estilo de vida_salud_kaggle.csv\")\n\n# Salidas:\nRUTA_DATASET_ENRIQ = Path(\"datos/dataset_enriquecido.csv\")\nRUTA_GRAFICOS = Path(\"docs/graficos\")\n\n# Variables clave seleccionadas (Etapa 1)\nVARS_CLAVE = [\n    \"age\",\n    \"bmi\",\n    \"exercise_days_per_week\",\n    \"sleep_hours\",\n    \"sugar_intake_g\"\n]\n\n# Opcional: manejo sencillo de outliers\nHANDLE_OUTLIERS = False  # cambia a True si quieres winsorizar por IQR\n\n# Crear carpetas destino si no existen\nRUTA_GRAFICOS.mkdir(parents=True, exist_ok=True)\nRUTA_DATASET_ENRIQ.parent.mkdir(parents=True, exist_ok=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Funciones auxiliares  \nSe definen funciones para:  \n- Normalizar nombres de columnas.  \n- Leer CSV con diferentes codificaciones.  \n- Mapear aliases de columnas de Kaggle a nombres est√°ndar.  \n- Reportar calidad de datos.  \n- Manejar nulos y (opcional) outliers.  \n- Generar/derivar fechas.  \n- Crear y guardar gr√°ficos.  \n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# =========================\n# Helpers de limpieza\n# =========================\ndef strip_accents(text: str) -> str:\n    \"\"\"Elimina acentos/diacr√≠ticos.\"\"\"\n    if not isinstance(text, str):\n        return text\n    text = unicodedata.normalize(\"NFD\", text)\n    text = \"\".join(ch for ch in text if unicodedata.category(ch) != \"Mn\")\n    return text\n\ndef normalize_column_name(col: str) -> str:\n    \"\"\"Normaliza nombre de columna: min√∫sculas, sin acentos, sin espacios, solo _.\"\"\"\n    col = strip_accents(col)\n    col = col.lower().strip()\n    col = re.sub(r\"\\s+\", \"_\", col)\n    col = re.sub(r\"[^a-z0-9_]\", \"\", col)\n    col = re.sub(r\"_+\", \"_\", col)\n    return col\n\ndef normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Aplica normalizaci√≥n a todas las columnas.\"\"\"\n    old_cols = df.columns.tolist()\n    df.columns = [normalize_column_name(c) for c in df.columns]\n    print(\"‚úÖ Columnas normalizadas.\")\n    print(\"Antes:\", old_cols)\n    print(\"Despu√©s:\", df.columns.tolist())\n    return df\n\ndef try_read_csv(path: Path) -> pd.DataFrame:\n    \"\"\"Lee CSV intentando codificaciones comunes.\"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"No se encontr√≥ el archivo en: {path.resolve()}\")\n    encodings = [\"utf-8\", \"latin-1\", \"cp1252\"]\n    for enc in encodings:\n        try:\n            df = pd.read_csv(path, encoding=enc)\n            print(f\"‚úÖ Dataset le√≠do con encoding {enc}. Shape: {df.shape}\")\n            return df\n        except Exception as e:\n            last_err = e\n    raise last_err\n\ndef map_aliases(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Mapea posibles variantes de nombres a est√°ndar.\n    √ötil si Kaggle trae columnas ligeramente distintas.\n    \"\"\"\n    aliases = {\n        \"age\": [\"age\", \"edad\"],\n        \"bmi\": [\"bmi\", \"imc\", \"indice_de_masa_corporal\"],\n        \"exercise_days_per_week\": [\n            \"exercise_days_per_week\", \"exercise_days\", \"exercise_frequency\",\n            \"dias_ejercicio_semana\", \"exercise_per_week\"\n        ],\n        \"sleep_hours\": [\"sleep_hours\", \"hours_sleep\", \"horas_sueno\", \"sleep_per_day\"],\n        \"sugar_intake_g\": [\n            \"sugar_intake_g\", \"sugar_intake\", \"sugar_g\", \"ingesta_azucar\",\n            \"sugar_consumption_g\"\n        ],\n    }\n\n    inverse = {}\n    for std, al_list in aliases.items():\n        for al in al_list:\n            inverse[al] = std\n\n    rename_dict = {}\n    for c in df.columns:\n        if c in inverse and inverse[c] != c:\n            rename_dict[c] = inverse[c]\n\n    if rename_dict:\n        df = df.rename(columns=rename_dict)\n        print(\"‚úÖ Aliases detectados y renombrados:\", rename_dict)\n    else:\n        print(\"‚ÑπÔ∏è No se detectaron aliases a renombrar.\")\n    return df\n\ndef report_missing(df: pd.DataFrame):\n    miss = df.isna().mean().sort_values(ascending=False)\n    print(\"\\nüìå Porcentaje de nulos por columna (top 10):\")\n    print((miss.head(10) * 100).round(2))\n\ndef coerce_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n    \"\"\"Convierte columnas a num√©ricas forzando errores a NaN.\"\"\"\n    for c in cols:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef handle_missing_key_vars(df: pd.DataFrame, cols: list, drop_thresh=0.05):\n    \"\"\"\n    Si nulos <= drop_thresh -> elimina filas.\n    Si nulos > drop_thresh -> imputa con mediana.\n    \"\"\"\n    for c in cols:\n        if c not in df.columns:\n            continue\n        ratio = df[c].isna().mean()\n        if ratio <= drop_thresh:\n            before = df.shape[0]\n            df = df.dropna(subset=[c])\n            after = df.shape[0]\n            print(f\"‚úÖ {c}: nulos {ratio:.2%} -> se eliminaron {before-after} filas.\")\n        else:\n            med = df[c].median()\n            df[c] = df[c].fillna(med)\n            print(f\"‚úÖ {c}: nulos {ratio:.2%} -> imputaci√≥n mediana={med:.3f}.\")\n    return df\n\ndef winsorize_iqr(df: pd.DataFrame, cols: list, k=1.5):\n    \"\"\"Recorta outliers con l√≠mites IQR.\"\"\"\n    for c in cols:\n        if c not in df.columns:\n            continue\n        q1 = df[c].quantile(0.25)\n        q3 = df[c].quantile(0.75)\n        iqr = q3 - q1\n        low = q1 - k * iqr\n        high = q3 + k * iqr\n        df[c] = df[c].clip(lower=low, upper=high)\n        print(f\"‚úÖ Outliers tratados en {c} con IQR.\")\n    return df\n\n# =========================\n# Enriquecimiento temporal\n# =========================\ndef detect_date_column(df: pd.DataFrame):\n    possibles = [\"date\", \"fecha\", \"fecha_registro\", \"recorded_date\", \"timestamp\"]\n    for c in possibles:\n        if c in df.columns:\n            return c\n    return None\n\ndef add_random_date(df: pd.DataFrame, start=\"2022-01-01\", end=\"2024-12-31\", seed=42):\n    \"\"\"Genera columna fecha aleatoria dentro del rango.\"\"\"\n    np.random.seed(seed)\n    start_dt = pd.to_datetime(start)\n    end_dt = pd.to_datetime(end)\n    days_range = (end_dt - start_dt).days\n\n    random_days = np.random.randint(0, days_range + 1, size=len(df))\n    df[\"fecha\"] = start_dt + pd.to_timedelta(random_days, unit=\"D\")\n    print(\"‚úÖ Columna 'fecha' generada aleatoriamente.\")\n    return df\n\ndef derive_date_parts(df: pd.DataFrame, date_col=\"fecha\"):\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df[\"anio\"] = df[date_col].dt.year\n    df[\"mes\"] = df[date_col].dt.month\n    df[\"dia\"] = df[date_col].dt.day\n    print(\"‚úÖ Columnas anio, mes, dia derivadas.\")\n    return df\n\n# =========================\n# EDA + gr√°ficos\n# =========================\ndef save_fig(path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=150)\n    plt.close()\n\ndef plot_histograms(df: pd.DataFrame, cols: list):\n    for c in cols:\n        if c not in df.columns:\n            continue\n        plt.figure()\n        df[c].hist(bins=30)\n        plt.title(f\"Histograma de {c}\")\n        plt.xlabel(c)\n        plt.ylabel(\"Frecuencia\")\n        save_fig(RUTA_GRAFICOS / f\"hist_{c}.png\")\n        print(f\"üìà Histograma guardado: hist_{c}.png\")\n\ndef plot_bar_exercise(df: pd.DataFrame):\n    c = \"exercise_days_per_week\"\n    if c not in df.columns:\n        return\n    plt.figure()\n    df[c].value_counts().sort_index().plot(kind=\"bar\")\n    plt.title(\"Frecuencia de d√≠as de ejercicio por semana\")\n    plt.xlabel(\"D√≠as por semana\")\n    plt.ylabel(\"N√∫mero de personas\")\n    save_fig(RUTA_GRAFICOS / \"bar_exercise_days_per_week.png\")\n    print(\"üìä Barras guardadas: bar_exercise_days_per_week.png\")\n\ndef plot_corr_heatmap(df: pd.DataFrame, cols: list):\n    data = df[cols].dropna()\n    corr = data.corr()\n\n    plt.figure()\n    plt.imshow(corr, interpolation=\"nearest\")\n    plt.colorbar()\n    plt.xticks(range(len(cols)), cols, rotation=45, ha=\"right\")\n    plt.yticks(range(len(cols)), cols)\n    plt.title(\"Matriz de correlaci√≥n (variables clave)\")\n    for i in range(len(cols)):\n        for j in range(len(cols)):\n            plt.text(j, i, f\"{corr.iloc[i, j]:.2f}\",\n                     ha=\"center\", va=\"center\")\n    save_fig(RUTA_GRAFICOS / \"corr_heatmap_vars_clave.png\")\n    print(\"üî• Heatmap guardado: corr_heatmap_vars_clave.png\")\n\ndef plot_time_series(df: pd.DataFrame, cols: list):\n    if \"anio\" not in df.columns:\n        return\n    agg = df.groupby(\"anio\")[cols].mean(numeric_only=True)\n\n    plt.figure()\n    for c in cols:\n        plt.plot(agg.index, agg[c], marker=\"o\", label=c)\n    plt.title(\"Promedio anual de variables clave\")\n    plt.xlabel(\"A√±o\")\n    plt.ylabel(\"Promedio\")\n    plt.legend()\n    save_fig(RUTA_GRAFICOS / \"ts_promedios_anuales.png\")\n    print(\"‚è±Ô∏è Serie temporal guardada: ts_promedios_anuales.png\")\n\ndef generar_interpretaciones(df: pd.DataFrame, cols: list):\n    \"\"\"Genera interpretaciones breves en Markdown para cada gr√°fico.\"\"\"\n    lines = []\n    lines.append(\"# Interpretaciones breves de gr√°ficos (Etapa 2)\\n\")\n\n    for c in [\"age\", \"bmi\", \"sleep_hours\", \"sugar_intake_g\"]:\n        if c not in df.columns:\n            continue\n        s = df[c].dropna()\n        lines.append(f\"## hist_{c}.png\")\n        lines.append(\n            f\"- Media: {s.mean():.2f}, mediana: {s.median():.2f}, desviaci√≥n: {s.std():.2f}.\\n\"\n            f\"- La distribuci√≥n se concentra alrededor de {s.median():.2f} \"\n            f\"con un rango aproximado entre {s.min():.2f} y {s.max():.2f}.\\n\"\n        )\n\n    if \"exercise_days_per_week\" in df.columns:\n        vc = df[\"exercise_days_per_week\"].value_counts().sort_index()\n        moda = vc.idxmax()\n        lines.append(\"## bar_exercise_days_per_week.png\")\n        lines.append(\n            f\"- El valor m√°s frecuente es {moda} d√≠as/semana.\\n\"\n            f\"- Se observa concentraci√≥n de participantes alrededor de ese nivel de actividad.\\n\"\n        )\n\n    corr = df[cols].corr()\n    max_pair = corr.where(~np.eye(len(cols), dtype=bool)).stack().idxmax()\n    max_val = corr.loc[max_pair]\n    lines.append(\"## corr_heatmap_vars_clave.png\")\n    lines.append(\n        f\"- La relaci√≥n m√°s fuerte se observa entre **{max_pair[0]}** y **{max_pair[1]}** \"\n        f\"con correlaci√≥n ‚âà {max_val:.2f}.\\n\"\n        \"- Esto sugiere asociaci√≥n entre h√°bitos/condiciones sin implicar causalidad.\\n\"\n    )\n\n    if \"anio\" in df.columns:\n        lines.append(\"## ts_promedios_anuales.png\")\n        lines.append(\n            \"- El gr√°fico temporal compara promedios por a√±o (simulado).\\n\"\n            \"- Permite observar tendencias generales de h√°bitos/condiciones en el periodo.\\n\"\n        )\n\n    out_path = RUTA_GRAFICOS / \"interpretaciones.md\"\n    out_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n    print(f\"üìù Interpretaciones guardadas en: {out_path}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Carga y limpieza de datos  \nEn esta secci√≥n se realiza:\n\n1. Carga del CSV original.  \n2. Normalizaci√≥n de nombres de columnas.  \n3. Eliminaci√≥n de duplicados.  \n4. Conversi√≥n de tipos a num√©ricos en variables clave.  \n5. Tratamiento de nulos con reglas expl√≠citas.  \n6. (Opcional) tratamiento de outliers.  \n\nCada decisi√≥n queda evidenciada con impresiones en consola.  \n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 1) Cargar dataset original\ndf = try_read_csv(RUTA_DATASET_ORIGINAL)\n\n# 2) Normalizar columnas\ndf = normalize_columns(df)\n\n# 3) Mapear aliases a nombres est√°ndar\ndf = map_aliases(df)\n\n# 4) Reporte inicial de calidad\nprint(\"\\nüìå Shape inicial:\", df.shape)\nreport_missing(df)\n\n# 5) Eliminar duplicados\ndup = df.duplicated().sum()\nif dup > 0:\n    df = df.drop_duplicates()\n    print(f\"‚úÖ Duplicados eliminados: {dup}\")\nelse:\n    print(\"‚ÑπÔ∏è No se encontraron duplicados.\")\n\n# 6) Conversi√≥n de tipos a num√©ricos en variables clave\ndf = coerce_numeric(df, VARS_CLAVE)\n\n# 7) Manejo de nulos en variables clave (drop si pocos, imputar si muchos)\ndf = handle_missing_key_vars(df, VARS_CLAVE, drop_thresh=0.05)\n\n# 8) Manejo opcional de outliers\nif HANDLE_OUTLIERS:\n    df = winsorize_iqr(df, VARS_CLAVE)\n\n# Reporte posterior a limpieza\nprint(\"\\nüìå Shape despu√©s de limpieza:\", df.shape)\nreport_missing(df)\n\ndf.head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Enriquecimiento temporal  \nSe verifica si existe columna de fecha.  \n- Si existe: se derivan `anio`, `mes`, `dia`.  \n- Si no existe: se genera una fecha aleatoria 2022‚Äì2024 y luego se derivan las columnas.  \n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "date_col = detect_date_column(df)\nif date_col:\n    print(f\"‚úÖ Columna fecha detectada: {date_col}\")\n    df = derive_date_parts(df, date_col)\nelse:\n    df = add_random_date(df, start=\"2022-01-01\", end=\"2024-12-31\", seed=42)\n    df = derive_date_parts(df, \"fecha\")\n\ndf[[\"fecha\", \"anio\", \"mes\", \"dia\"]].head()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Exportaci√≥n del dataset enriquecido  \nEl dataset resultante se guarda como:  \n`datos/dataset_enriquecido.csv`  \n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "df.to_csv(RUTA_DATASET_ENRIQ, index=False)\nprint(f\"‚úÖ Dataset enriquecido guardado en: {RUTA_DATASET_ENRIQ}\")\n\nRUTA_DATASET_ENRIQ\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Estad√≠sticas descriptivas (describe)  \nSe calculan estad√≠sticas generales para las 5 variables clave y se exporta un CSV de evidencia.  \n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "describe_df = df[VARS_CLAVE].describe()\ndescribe_df\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "describe_path = RUTA_GRAFICOS / \"describe_selected.csv\"\ndescribe_df.to_csv(describe_path)\nprint(f\"üßæ Describe guardado en: {describe_path}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Visualizaciones EDA  \nSe generan y guardan en `docs/graficos/`:\n\n- Histogramas: `age`, `bmi`, `sleep_hours`, `sugar_intake_g`.  \n- Barras: `exercise_days_per_week`.  \n- Correlaci√≥n: heatmap con las 5 variables clave.  \n- Temporal: promedios por a√±o.  \n\n> Despu√©s de ejecutar, revisa la carpeta `docs/graficos/` y copia/pega las interpretaciones en tu documento APA.  \n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "plot_histograms(df, [\"age\", \"bmi\", \"sleep_hours\", \"sugar_intake_g\"])\nplot_bar_exercise(df)\nplot_corr_heatmap(df, VARS_CLAVE)\nplot_time_series(df, VARS_CLAVE)\n\nprint(\"‚úÖ Gr√°ficos generados. Revisa docs/graficos/\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Interpretaciones breves  \nPara cumplir la r√∫brica, se genera autom√°ticamente un archivo con interpretaciones cortas:  \n`docs/graficos/interpretaciones.md`  \n\nPuedes usar ese texto ‚Äúdebajo de cada gr√°fico‚Äù en el notebook o en el documento APA.  \n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "generar_interpretaciones(df, VARS_CLAVE)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## 9. Conclusiones parciales de Etapa 2  \n(Escribe aqu√≠ 1‚Äì2 p√°rrafos cortos sobre qu√© encontraste en las 5 variables clave y qu√© tendencias/relaciones te parecen importantes.)\n\n- ‚úÖ Limpieza y normalizaci√≥n completadas.  \n- ‚úÖ Columnas fecha, anio, mes y dia agregadas.  \n- ‚úÖ Dataset enriquecido exportado.  \n- ‚úÖ EDA y gr√°ficos generados con evidencia.  \n- ‚úÖ Interpretaciones listas para el APA.  \n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}